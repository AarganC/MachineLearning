{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, BatchNormalization, concatenate, multiply, add, Activation, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2 \n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=2\n",
    "lera=0.001\n",
    "activation=\"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input1 = np.memmap(\"../Data/train-input_1.npy\", shape=(24000000, 884))\n",
    "train_input2 = np.memmap(\"../Data/train-input_2.npy\", shape=(24000000, 260))\n",
    "train_output1 = np.memmap(\"../Data/train-output_1.npy\", shape=(24000000, 260))\n",
    "train_output2 = np.memmap(\"../Data/train-output_2.npy\", shape=(24000000, 1))\n",
    "validation_input_1 = np.memmap(\"../Data/validation-input_1.npy\", shape=(6000000, 884))\n",
    "validation_input_2 = np.memmap(\"../Data/validation-input_2.npy\", shape=(6000000, 260))\n",
    "validation_output_1 = np.memmap(\"../Data/validation-output_1.npy\", shape=(6000000, 260))\n",
    "validation_output_2 = np.memmap(\"../Data/validation-output_2.npy\", shape=(6000000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input1[[147  78  85 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "train_input1 shape(24000000, 884)\n",
      "train_input2[[147  78  85 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "train_input2 shape(24000000, 260)\n",
      "train_output1[[147  78  85 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "train_output1 shape(24000000, 260)\n",
      "train_output2[[147]\n",
      " [ 78]\n",
      " [ 85]\n",
      " ...\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]]\n",
      "train_output2 shape(24000000, 1)\n",
      "validation_input_1[[147  78  85 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "validation_input_1 shape(6000000, 884)\n",
      "validation_input_2[[147  78  85 ...   0   0   0]\n",
      " [  0   0 128 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "validation_input_2 shape(6000000, 260)\n",
      "validation_output_1[[147  78  85 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "validation_output_1(6000000, 260)\n",
      "validation_output_2[[147]\n",
      " [ 78]\n",
      " [ 85]\n",
      " ...\n",
      " [  0]\n",
      " [128]\n",
      " [ 63]]\n",
      "validation_output_2 shape(6000000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_input1\" + str(train_input1))\n",
    "print(\"train_input1 shape\" + str(train_input1.shape))\n",
    "print(\"train_input2\" + str(train_input2))\n",
    "print(\"train_input2 shape\" + str(train_input2.shape))\n",
    "print(\"train_output1\" + str(train_output1))\n",
    "print(\"train_output1 shape\" + str(train_output1.shape))\n",
    "print(\"train_output2\" + str(train_output2))\n",
    "print(\"train_output2 shape\" + str(train_output2.shape))\n",
    "print(\"validation_input_1\" + str(validation_input_1))\n",
    "print(\"validation_input_1 shape\" + str(validation_input_1.shape))\n",
    "print(\"validation_input_2\" + str(validation_input_2))\n",
    "print(\"validation_input_2 shape\" + str(validation_input_2.shape))\n",
    "print(\"validation_output_1\" + str(validation_output_1))\n",
    "print(\"validation_output_1\" + str(validation_output_1.shape))\n",
    "print(\"validation_output_2\" + str(validation_output_2))\n",
    "print(\"validation_output_2 shape\" + str(validation_output_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 12:26:10.354666 139742283507456 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 12:26:10.396006 139742283507456 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#input_1_shape = train_input1.shape\n",
    "#input_2_shape = train_input2.shape\n",
    "output_1_shape = train_output1.shape\n",
    "output_2_shape = train_output2.shape\n",
    "\n",
    "input_1_shape = 884,\n",
    "input_2_shape = 260,\n",
    "\n",
    "input_1 = Input(shape=(884,), name=\"intput_1\")\n",
    "input_2 = Input(shape=(260,), name=\"intput_2\")\n",
    "output_1 = Dense(260, activation='softmax', name=\"output_1\")(input_1 )\n",
    "output_2 = Dense(1, activation='sigmoid', name=\"output_2\")(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This embedding layer will encode the input sequence\n",
    "C = Embedding(10000,  32 ,  input_length = input_1_shape)(input_1)\n",
    "H = Embedding(10000,  16 ,  input_length = input_1_shape)(input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layer=3\n",
    "act='selu'\n",
    "n=128\n",
    "\n",
    "\n",
    "inputs = Input(shape=train[0].shape)\n",
    "x=Dense(n, activation=act)(inputs)\n",
    "for i in range(nb_layer):\n",
    "    x=Dense(n, activation=act)(x)\n",
    "output=Dense(2, activation='sigmoid')(x)\n",
    "sgd = SGD(lr=0.1)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer flatten_5: expected min_ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c0b8b5048ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# And finally we add the main logistic regression layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#main_output = Dense(1, activation='sigmoid', name='output_1')(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mauxiliary_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected min_ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer flatten_5: expected min_ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "inputs = input_1\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "#main_output = Dense(1, activation='sigmoid', name='output_1')(x)\n",
    "x = Flatten()(x)\n",
    "y = Flatten()(x)\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='output_2')(y)\n",
    "\n",
    "test = Embedding(10000,  32 ,  input_length = (260))(input_2)\n",
    "main_output = Dense(260, activation='softmax', name=\"output_1\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cee2296ad6a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mauxiliary_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "y = Flatten()(x)\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='output_2')(y)\n",
    "\n",
    "test = Embedding(10000,  32 ,  input_length = (260))(input_2)\n",
    "x = concatenate([x, test], axis=1)\n",
    "x=Dense(128, activation=act)(inputs)\n",
    "x=Dense(128, activation=act)(x)\n",
    "main_output = Dense(260, activation='softmax', name=\"output_1\")(x)\n",
    "sgd = SGD(lr=0.1)\n",
    "\n",
    "model = Model(inputs=[input_1, input_2], outputs=[main_output, auxiliary_output])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss={'output_1': 'binary_crossentropy', 'output_2': 'binary_crossentropy'},\n",
    "              loss_weights={'output_1': 1.0, 'output_2': 0.001})\n",
    "\n",
    "# And trained it via:\n",
    "#model.fit_generator({'main_input': train_input1, 'aux_input': train_input2},\n",
    "#                      {'main_output': train_output1, 'aux_output': train_output2},\n",
    "#                      samples_per_epoch=10000, steps_per_epoch=(train_samples/ batch_size),)\n",
    "                        \n",
    "#X_out = np.concatenate([train_output1, train_output2])\n",
    "#r=len(train_output1)+len(train_output2)\n",
    "#X_out = np.memmap(\"../Data/train-output_1.npy\", shape=(r), mode='r+')\n",
    "#X_out[len(train_output1):] = train_output2\n",
    "\n",
    "#r=len(train_input1)+len(train_input2)\n",
    "#X_in = np.memmap(\"../Data/train-input_1.npy\", shape=(r), mode='r+')\n",
    "#X_in[len(train_input1):] = train_input2\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'res_logs')\n",
    "date = datetime.today()\n",
    "year = date.strftime(\"%Y\")\n",
    "month = date.strftime(\"%m\")\n",
    "day = date.strftime(\"%d\")\n",
    "hour = date.strftime(\"%H\")\n",
    "minute = date.strftime(\"%M\")\n",
    "model_name = \"{}{}{}{}{}_test\" \\\n",
    "    .format(year, month, day, hour, minute)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "callbacks = TensorBoard(log_dir=filepath)\n",
    "\n",
    "#model.fit_generator([train_input1, train_input2], steps_per_epoch=15, epochs=1, verbose=1, \n",
    "#              callbacks=None, validation_data=X_out, \n",
    "#              validation_steps=None, class_weight=None, max_queue_size=10, workers=3, use_multiprocessing=True, \n",
    "#              shuffle=True, initial_epoch=0)\n",
    "\n",
    "model.fit([train_input1, train_input2],\n",
    "          [train_output1, train_output2],\n",
    "          epochs=10, batch_size=1024, callbacks=[callbacks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
