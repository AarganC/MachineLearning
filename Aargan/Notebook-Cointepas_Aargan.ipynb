{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, BatchNormalization, concatenate, multiply, add, Activation, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètrage de l'utilisation GPU\n",
    "Les notebooks sont limité à 50% d'utilisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66.  84.]\n",
      " [147. 192.]]\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "for d in ['/gpu:1', '/gpu:2', '/gpu:3']:\n",
    "  with tf.device(d):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "    c.append(tf.matmul(a, b))\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "#sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Paramètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=2\n",
    "lera=0.001\n",
    "activation=\"relu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input1 = np.load(\"../Data/train-input_1.npy\")\n",
    "train_input2 = np.load(\"../Data/train-input_2.npy\")\n",
    "train_output1 = np.load(\"../Data/train-output_1.npy\")\n",
    "train_output2 = np.load(\"../Data/train-output_2.npy\")\n",
    "validation_input_1 = np.load(\"../Data/validation-input_1.npy\")\n",
    "validation_input_2 = np.load(\"../Data/validation-input_2.npy\")\n",
    "validation_output_1 = np.load(\"../Data/validation-output_1.npy\")\n",
    "validation_output_2 = np.load(\"../Data/validation-output_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input1[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_input1 shape(24000000, 884)\n",
      "train_input2[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_input2 shape(24000000, 260)\n",
      "train_output1[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_output1 shape(24000000, 260)\n",
      "train_output2[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "train_output2 shape(24000000, 1)\n",
      "validation_input_1[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 1.]]\n",
      "validation_input_1 shape(6000000, 884)\n",
      "validation_input_2[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "validation_input_2 shape(6000000, 260)\n",
      "validation_output_1[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "validation_output_1(6000000, 260)\n",
      "validation_output_2[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "validation_output_2 shape(6000000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_input1\" + str(train_input1))\n",
    "print(\"train_input1 shape\" + str(train_input1.shape))\n",
    "print(\"train_input2\" + str(train_input2))\n",
    "print(\"train_input2 shape\" + str(train_input2.shape))\n",
    "print(\"train_output1\" + str(train_output1))\n",
    "print(\"train_output1 shape\" + str(train_output1.shape))\n",
    "print(\"train_output2\" + str(train_output2))\n",
    "print(\"train_output2 shape\" + str(train_output2.shape))\n",
    "print(\"validation_input_1\" + str(validation_input_1))\n",
    "print(\"validation_input_1 shape\" + str(validation_input_1.shape))\n",
    "print(\"validation_input_2\" + str(validation_input_2))\n",
    "print(\"validation_input_2 shape\" + str(validation_input_2.shape))\n",
    "print(\"validation_output_1\" + str(validation_output_1))\n",
    "print(\"validation_output_1\" + str(validation_output_1.shape))\n",
    "print(\"validation_output_2\" + str(validation_output_2))\n",
    "print(\"validation_output_2 shape\" + str(validation_output_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 13:18:23.020337 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 13:18:23.024506 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#input_1_shape = train_input1.shape\n",
    "#input_2_shape = train_input2.shape\n",
    "output_1_shape = train_output1.shape\n",
    "output_2_shape = train_output2.shape\n",
    "\n",
    "input_1_shape = 884,\n",
    "input_2_shape = 260,\n",
    "\n",
    "input_1 = Input(shape=(884,), name=\"intput_1\")\n",
    "input_2 = Input(shape=(260,), name=\"intput_2\")\n",
    "output_1 = Dense(260, activation='softmax', name=\"output_1\")(input_1 )\n",
    "output_2 = Dense(1, activation='sigmoid', name=\"output_2\")(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 13:18:23.054775 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This embedding layer will encode the input sequence\n",
    "C = Embedding(10000,  32 ,  input_length = input_1_shape)(input_1)\n",
    "H = Embedding(10000,  16 ,  input_length = input_1_shape)(input_1)\n",
    "#lstm_out = LSTM(32, return_sequences=True, input_shape=(-1, 24000000, 884, 32))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "\n",
    "Hx = concatenate([H, H])\n",
    "F = Activation('sigmoid')(Hx)\n",
    "\n",
    "I = Activation('sigmoid')(Hx)\n",
    "L = Activation('tanh')(Hx)\n",
    "IL = multiply([I, L])\n",
    "\n",
    "C = multiply([C, F])\n",
    "C = add([C, IL])\n",
    "\n",
    "C = Activation('tanh')(C)\n",
    "O = Activation('sigmoid')(Hx)\n",
    "x = multiply([C, O])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Flatten()(x)\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='output_2')(y)\n",
    "\n",
    "test = Embedding(10000,  32 ,  input_length = (260))(input_2)\n",
    "\n",
    "x = concatenate([x, test], axis=1)\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "#main_output = Dense(1, activation='sigmoid', name='output_1')(x)\n",
    "x = Flatten()(x)\n",
    "main_output = Dense(260, activation='softmax', name=\"output_1\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_1, input_2], outputs=[main_output, auxiliary_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 13:18:23.213797 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 13:18:23.231926 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 13:18:23.236143 140558498883328 deprecation.py:323] From /home/aargancointepas/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 13:18:26.658596 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000000 samples, validate on 6000000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 13:18:28.371874 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0714 13:18:28.373144 140558498883328 deprecation_wrapper.py:119] From /home/aargancointepas/.local/lib/python3.5/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 8777728/24000000 [=========>....................] - ETA: 32:08 - loss: 0.0137 - output_1_loss: 0.0130 - output_2_loss: 0.6865"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'output_1': 'binary_crossentropy', 'output_2': 'binary_crossentropy'},\n",
    "              loss_weights={'output_1': 1.0, 'output_2': 0.001})\n",
    "\n",
    "# And trained it via:\n",
    "#model.fit_generator({'main_input': train_input1, 'aux_input': train_input2},\n",
    "#                      {'main_output': train_output1, 'aux_output': train_output2},\n",
    "#                      samples_per_epoch=10000, steps_per_epoch=(train_samples/ batch_size),)\n",
    "                        \n",
    "#X_out = np.concatenate([train_output1, train_output2])\n",
    "#r=len(train_output1)+len(train_output2)\n",
    "#X_out = np.memmap(\"../Data/train-output_1.npy\", shape=(r), mode='r+')\n",
    "#X_out[len(train_output1):] = train_output2\n",
    "\n",
    "#r=len(train_input1)+len(train_input2)\n",
    "#X_in = np.memmap(\"../Data/train-input_1.npy\", shape=(r), mode='r+')\n",
    "#X_in[len(train_input1):] = train_input2\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'res_logs')\n",
    "date = datetime.today()\n",
    "year = date.strftime(\"%Y\")\n",
    "month = date.strftime(\"%m\")\n",
    "day = date.strftime(\"%d\")\n",
    "hour = date.strftime(\"%H\")\n",
    "minute = date.strftime(\"%M\")\n",
    "model_name = \"{}{}{}{}{}_test\" \\\n",
    "    .format(year, month, day, hour, minute)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "callbacks = TensorBoard(log_dir=filepath)\n",
    "\n",
    "#model.fit_generator([train_input1, train_input2], steps_per_epoch=15, epochs=1, verbose=1, \n",
    "#              callbacks=None, validation_data=X_out, \n",
    "#              validation_steps=None, class_weight=None, max_queue_size=10, workers=3, use_multiprocessing=True, \n",
    "#              shuffle=True, initial_epoch=0)\n",
    "\n",
    "model.fit([train_input1, train_input2],\n",
    "          [train_output1, train_output2],\n",
    "          epochs=10, batch_size=4096, callbacks=[callbacks],\n",
    "         validation_data=([validation_input_1, validation_input_2], [validation_output_1, validation_output_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# bashCommand = \"gcloud compute instances stop instance-2 -q --zone europe-west4-a\"\n",
    "# process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "# output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
